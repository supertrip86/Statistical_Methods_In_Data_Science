edge_connectivity(selected_grph)    # Connectivity measure for the edges
vertex_connectivity(selected_grph)  # Connectivity measure for the vertices
head(betweenness(selected_grph, directed = TRUE),10)   # Vertices betweenness, first 10 values
head(edge_betweenness(selected_grph, directed = TRUE),10)   # Edges betweenness, first 10 values
skewness = function(x) {
mean( (x - mean(x))^3 ) / sd(x)^3
}
z = degree(selected_grph)
print(skewness(z))
print(mean(z) > median(z))
tr <- make_tree(40, children = 3, mode = "undirected")
plot(tr, vertex.size=10, vertex.label=NA)
transitivity(tr)
edge_connectivity(tr)
vertex_connectivity(tr)
skewness = function(x) {
mean( (x - mean(x))^3 ) / sd(x)^3
}
z = degree(selected_grph, mode="in")
print(skewness(z))
print(mean(z) > median(z))
suppressPackageStartupMessages(library(igraph))
suppressPackageStartupMessages(library(ggraph))
suppressPackageStartupMessages(library(sdpt3r))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(poweRlaw))
# Defining the nodes of the graph
graph_nodes <- data.frame(node=c(1,2,3,4,5,6,7))
# Defining the edges of the graph
graph_edges <- data.frame(from=c(1,1,2,4,3,5,5), to=c(2,4,3,3,5,6,7))
# Defining the graph
g <- igraph::graph_from_data_frame(graph_edges, directed=FALSE, vertices=graph_nodes)
# Plotting the graph
plot(g, vertex.size=25, vertex.color="orchid", main="Undirected graph with 7 nodes and 7 edges")
# Calculating the starting time of the random max-cut algorithm over the small graph with 7 nodes and 7 edges
startTime <- Sys.time()
# Pre-allocating a vector proportional to the number of the nodes of the graph
vecOfOutcomes <- rep(NA, nrow(graph_nodes))
# Inserting the outcomes, simulated from a coin toss with binomial distribution, into a vector
for (i in 1:nrow(graph_nodes)){
outcome <- rbinom(1, 1, 0.5)
vecOfOutcomes[i] <- outcome
}
# Getting the size of set U (U has to be created in the next lines)
count <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
count <- count + 1
}
}
# Pre-allocating and creating the U set randomly, based on the previuos binomial simulation
U <- rep(NA, count)
indexOfU <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
indexOfU <- indexOfU + 1
U[indexOfU] <- graph_nodes[i,1]
}
}
# Check if the edges are inside set U (XOR conditioning) and get the cut
cut <- 0
for (row in 1:nrow(graph_edges)) {
first <- graph_edges[row, "from"]
second  <- graph_edges[row, "to"]
if (xor(first %in% U,second %in% U)){
cut <- cut + 1
}
}
# Converting the previous graph into a symmetric adjacency matrix
matrix <- as.matrix(as_adjacency_matrix(g,type = c("both")))
# Applying the maxcut algorithm of the package sdpt3r
OPT <- round(maxcut(matrix)$pobj)
OPT <- abs(OPT)
# Choosing M equal to 10 000
M = 10000
# Copying and pasting the random algorithm implemented previously and simulating it M times
cutValues <- rep(NA, M)
# Run the randomized algorithm M times
for (j in 1:M){
# Inserting the outcomes, simulated from a coin toss with binomial distribution, into the vector
for (i in 1:nrow(graph_nodes)){
outcome <- rbinom(1, 1, 0.5)
vecOfOutcomes[i] <- outcome
}
# Getting the size of set U (to be created)
count <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
count <- count + 1
}
}
# Pre-allocating and creating the U set randomly, based on the previuos binomial simulation
U <- rep(NA, count)
indexOfU <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
indexOfU <- indexOfU + 1
U[indexOfU] <- graph_nodes[i,1]
}
}
# Check if the edges are inside set U (XOR conditioning) and get the cut
cut <- 0
for (row in 1:nrow(graph_edges)) {
first <- graph_edges[row, "from"]
second  <- graph_edges[row, "to"]
if (xor(first %in% U,second %in% U)){
cut <- cut + 1
}
}
# Storing each cut-size in a vector
cutValues[j] <- cut
}
# Calculating the ending time of the random max-cut algorithm over the small graph with 7 nodes and 7 edges, simulated M times
endTime <- Sys.time()
# Calculating the running time of the random max-cut algorithm simulated M times over the small graph with 7 nodes and 7 edges
deltaTime <- endTime-startTime
# Calculating the average size-cut
meanCut <- mean(cutValues)
# Calculating the maximum size-cut
maxCut <- max(cutValues)
# Calculating the minimum size-cut
minCut <- min(cutValues)
# Calculating the standard deviation of the size-cut
stanDev <- sd(cutValues)
# Calculating the coefficient of variation
coeffVariation <- round(stanDev/meanCut,2)
# Drawing the histogram of the average cut-size values of the randomized max-cut algorithm
hist(cutValues, main = paste("Histogram of average cut-size of the randomized max-cut algorithm"), xlab="Size of the cut", col="orchid", breaks=nrow(graph_edges))
# Creating a random graph and plotting it
randomGraph <- sample_gnm(100, 20, directed = FALSE, loops = FALSE)
lay <- layout_with_lgl(randomGraph)
plot(randomGraph, vertex.size=25, vertex.color="orchid", main="Undirected graph with 100 nodes and 20 edges", layout=lay)
# Getting the nodes of the graph and converting it into a dataframe
graph_nodes <- data.frame(node=c(as_ids(V(randomGraph))))
# Getting the edges of the graph and converting it into a renaimed dataframe
graph_edges <- data.frame(as_edgelist(randomGraph))
colnames(graph_edges) <- c("from","to")
# Calculating the starting time of the random max-cut algorithm over the graph with 100 nodes and 20 edges
startTime <- Sys.time()
# Pre-allocating a vector proportional to the number of the nodes of the graph
vecOfOutcomes <- rep(NA, nrow(graph_nodes))
# Inserting the outcomes, simulated from a coin toss with binomial distribution, into the vector
for (i in 1:nrow(graph_nodes)){
outcome <- rbinom(1, 1, 0.5)
vecOfOutcomes[i] <- outcome
}
# Getting the size of set U (to be created)
count <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
count <- count + 1
}
}
# Pre-allocating and creating the U set randomly, based on the previuos binomial simulation
U <- rep(NA, count)
indexOfU <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
indexOfU <- indexOfU + 1
U[indexOfU] <- graph_nodes[i,1]
}
}
# Get the true OPT(G) or at very least a good approximation to OPT(G)
matrix <- as.matrix(as_adjacency_matrix(randomGraph,type = c("both")))
OPT <- round(maxcut(matrix)$pobj)
OPT <- abs(OPT)
# Check if the edges are inside set U (XOR conditioning) and get the cut
cut <- 0
for (row in 1:nrow(graph_edges)) {
first <- graph_edges[row, "from"]
second  <- graph_edges[row, "to"]
if (xor(first %in% U,second %in% U)){
cut <- cut + 1
}
}
# Running the Randomized Max-Cut Algorithm a large number M of times
M = 10000
cutValues <- rep(NA, M)
for (j in 1:M){
# Inserting the outcomes, simulated from a coin toss with binomial distribution, into the vector
for (i in 1:nrow(graph_nodes)){
outcome <- rbinom(1, 1, 0.5)
vecOfOutcomes[i] <- outcome
}
# Getting the size of set U (to be created)
count <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
count <- count + 1
}
}
# Pre-allocating and creating the U set randomly, based on the previuos binomial simulation
U <- rep(NA, count)
indexOfU <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
indexOfU <- indexOfU + 1
U[indexOfU] <- graph_nodes[i,1]
}
}
# Check if the edges are inside set U (XOR conditioning) and get the cut
cut <- 0
for (row in 1:nrow(graph_edges)) {
first <- graph_edges[row, "from"]
second  <- graph_edges[row, "to"]
if (xor(first %in% U,second %in% U)){
cut <- cut + 1
}
}
# Storing each cut value
cutValues[j] <- cut
}
# Calculating the ending time of the random max-cut algorithm over the graph with 100 nodes and 20 edges, simulated M times
endTime <- Sys.time()
# Calculating the running time of the random max-cut algorithm simulated M times over the graph with 100 nodes and 20 edges
deltaTime <- endTime-startTime
# Some analysis over the cut-sized obtained from the M simulations
meanCut <- mean(cutValues)
maxCut <- max(cutValues)
minCut <- min(cutValues)
stanDev <- sd(cutValues)
coeffVariation <- round(stanDev/meanCut,2)
hist(cutValues, main = paste("Histogram of average cut-size of the randomized max-cut algorithm"), xlab="Size of the cut", col="orchid", breaks=nrow(graph_edges))
# Creating a random graph and plotting it
randomGraph <- sample_gnm(100, 90, directed = FALSE, loops = FALSE)
lay <- layout_with_lgl(randomGraph)
plot(randomGraph, vertex.size=25, vertex.color="orchid", main="Undirected graph with 100 nodes and 90 edges", layout=lay)
# Getting the nodes of the graph and converting it into a dataframe
graph_nodes <- data.frame(node=c(as_ids(V(randomGraph))))
# Getting the edges of the graph and converting it into a renaimed dataframe
graph_edges <- data.frame(as_edgelist(randomGraph))
colnames(graph_edges) <- c("from","to")
# Calculating the starting time of the random max-cut algorithm over the graph with 100 nodes and 90 edges
startTime <- Sys.time()
# Pre-allocating a vector proportional to the number of the nodes of the graph
vecOfOutcomes <- rep(NA, nrow(graph_nodes))
# Inserting the outcomes, simulated from a coin toss with binomial distribution, into the vector
for (i in 1:nrow(graph_nodes)){
outcome <- rbinom(1, 1, 0.5)
vecOfOutcomes[i] <- outcome
}
# Getting the size of set U (to be created)
count <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
count <- count + 1
}
}
# Pre-allocating and creating the U set randomly, based on the previuos binomial simulation
U <- rep(NA, count)
indexOfU <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
indexOfU <- indexOfU + 1
U[indexOfU] <- graph_nodes[i,1]
}
}
# Get the true OPT(G) or at very least a good approximation to OPT(G)
matrix <- as.matrix(as_adjacency_matrix(randomGraph,type = c("both")))
OPT <- round(maxcut(matrix)$pobj)
OPT <- abs(OPT)
# Check if the edges are inside set U (XOR conditioning) and get the cut
cut <- 0
for (row in 1:nrow(graph_edges)) {
first <- graph_edges[row, "from"]
second  <- graph_edges[row, "to"]
if (xor(first %in% U,second %in% U)){
cut <- cut + 1
}
}
# Running the Randomized Max-Cut Algorithm a large number M of times
M = 10000
cutValues <- rep(NA, M)
for (j in 1:M){
# Inserting the outcomes, simulated from a coin toss with binomial distribution, into the vector
for (i in 1:nrow(graph_nodes)){
outcome <- rbinom(1, 1, 0.5)
vecOfOutcomes[i] <- outcome
}
# Getting the size of set U (to be created)
count <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
count <- count + 1
}
}
# Pre-allocating and creating the U set randomly, based on the previuos binomial simulation
U <- rep(NA, count)
indexOfU <- 0
for (i in 1:length(vecOfOutcomes)){
if (vecOfOutcomes[i] == 1){
indexOfU <- indexOfU + 1
U[indexOfU] <- graph_nodes[i,1]
}
}
# Check if the edges are inside set U (XOR conditioning) and get the cut
cut <- 0
for (row in 1:nrow(graph_edges)) {
first <- graph_edges[row, "from"]
second  <- graph_edges[row, "to"]
if (xor(first %in% U,second %in% U)){
cut <- cut + 1
}
}
# Storing each cut value
cutValues[j] <- cut
}
# Calculating the ending time of the random max-cut algorithm over the graph with 100 nodes and 90 edges, simulated M times
endTime <- Sys.time()
# Calculating the running time of the random max-cut algorithm simulated M times over the graph with 100 nodes and 90 edges
deltaTime <- endTime-startTime
# Some analysis over the cut-sized obtained from the M simulations
meanCut <- mean(cutValues)
maxCut <- max(cutValues)
minCut <- min(cutValues)
stanDev <- sd(cutValues)
coeffVariation <- round(stanDev/meanCut,2)
hist(cutValues, main = paste("Histogram of average cut-size of the randomized max-cut algorithm"), xlab="Size of the cut", col="orchid", breaks=nrow(graph_edges))
get_random_integer = function(value) {
# This function picks randomly (uniformly) one of the pages that have been added so
# far to our network simulation.
# Since every page is labelled as an integer going from 1 to the current iteration
# value, we simply generate a random number belonging to such interval. To avoid
# the very small chance that the link will point to the page itself, we simply
# consider "max=value" rather than "max=value + 1".
result = floor(runif(1, min=1, max=value))
return (result)
}
get_random_link = function(m, value) {
# In this case we select randomly (uniformly) a single link out of the vector "m".
# In "m" we have inserted every link SO FAR created (until the current iteration
# "i", which produced the edge labelled as "value", with i == value), therefore
# we have a much higher chance to select from "m" a link to a page that has a
# high number of links attached to it. Since the the current "value" is not in
# the vector "m", the fact that the generated link will be an "outlink" is guaranteed.
links = to_vector(m)
maxVal = length(links) + 1
result = floor(runif(1, min=1, max=maxVal))
return (links[result])
}
to_vector = function(listVar) {
# This is a simple function that converts any list to a vector.
return (unlist(listVar, use.names=FALSE))
}
set_initial_edges = function() {
# This is a container for the commands to generate the starting graph
# containing 4 edges connected to each other as a directed cycle on 4 vertices.
loop = rep(1:4, each = 2)
vertices = c(loop[-1], loop[1])
return (vertices)
}
create_graph_data = function(start, end, gamma) {
# This is the function that generates a random edge for each newly created vertex.
# Each vertex is labelled as an integer going from "start" to "end", and can be
# considered as an additional page added iteratively to our simulation model. Our
# goal is to associate to each page a single link (edge) to a different page
# (vertex). Of course, the link has to point to a different page, not to itself.
# We proceed as follows: we create two different lists, "l" and "m". In "l" we
# append, at every iteration, the newly created edge AND a randomly generated
# link pointing to a different page (outlink), while in "m" we append only the
# randomly generated link.
# This link will be either:
#
# 1- Created so that it picks randomly one of the pages SO FAR existing in the graph
#    (added up to the current iteration), with probability gamma.
#
# 2- Copied among the elements of the list "m", containing all the links existing
#    SO FAR (added to the graph up to the current iteration), with probability
#    1 - gamma. Since every existing link is in "m", duplications included, it is way
#    more probable to copy one of the links pointing to a popular page than to copy any
#    of those few links pointing to a rarely referenced page.
#
# At the end of the iteration, we are interested in the list "l" containing the sequence
# of all the edges, going from vertex "i" to vertex "x", for every "i" and "x" belonging
# to "l". Eventually, we convert "l" to a vector and return the result.
l = list()
m = list(1,2,3,4)
for (i in seq(start,end)) {
p = rbernoulli(1, p = gamma)
if (p) {
x = get_random_integer(i)
} else {
x = get_random_link(m, i)
}
l = append(list(i,x), l)
m = append(list(x), m)
}
data = to_vector(l)
return (data)
}
generate_graph = function(end) {
gamma = 0.5   # the probability of choosing a link at random among all the pages
initial_vertices = 4   # the number of vertices of the initial graph
initial_edges = set_initial_edges()   # the edges of the initial graph
start = initial_vertices + 1    # the starting value of the interval
increment = end - start + 1   # the number of vertices to add to the initial graph
data = create_graph_data(start, end, gamma)   # creates an edge for each additional vertex
grph <- make_empty_graph() %>%          # initialize an empty graph
add_vertices(initial_vertices) %>%    # add the first 4 vertices
add_edges(initial_edges) %>%          # add the first 4 edges
add_vertices(increment) %>%           # add to the initial graph the remaining vertices
add_edges(data)                       # add to the initial graph the remaining edges
return (grph)
}
a = generate_graph(10000)
plot(
a,
edge.width=1,
edge.arrow.size=0.1,
vertex.size=1,
vertex.label.cex=0.1
)
title(
"Randomly generated network with a Preferential Attachment Process \n and 10.000 nodes",
line = -19
)
b = generate_graph(100000)
plot(
b,
edge.width=1,
edge.arrow.size=0.1,
vertex.size=1,
vertex.label.cex=0.1
)
title(
"Randomly generated network with a Preferential Attachment Process \n and 100.000 nodes",
line = -19
)
c = generate_graph(100000)
d = generate_graph(100000)
e = generate_graph(100000)
f = generate_graph(100000)
g = generate_graph(100000)
graph_list = list(b,c,d,e,f,g)
options(scipen = 999)
par(mfrow=c(2,3))
for (grph in graph_list) {
# we use the {igraph} built-in method to count the degree of each node.
degrees = degree(grph, mode="in")
plot(degrees, xlab="Nodes", ylab="Number of vertices")
}
title('Network degree distributions', line=-1, outer=TRUE)
par(mfrow=c(2,3))
for (grph in graph_list) {
# we use the {igraph} built-in method to count the degree of each node.
degrees = degree(grph, mode="in")
filteredDegrees = degrees[degrees > 0]
plot(filteredDegrees, log="xy", xlab="Nodes", ylab="Number of vertices")
}
title('Network degree distributions (log - log)', line=-1, outer=TRUE)
selected_grph = graph_list[[1]] # We pick one of the graphs in the list
ccdd_dist = function(deg) {
dg1 = list();
for (i in 1:max(deg)) {
dg1[i] = length(deg[deg >= i]) / length(deg);
}
return (dg1)
}
degrees = degree(selected_grph, mode="in")
filteredDegrees = degrees[degrees > 0]
dg1 = ccdd_dist(filteredDegrees)
dg2 = degree_distribution(selected_grph, cumulative=TRUE, mode="in")
dg2 = dg2[dg2 > 0]
options(scipen = 0)
plot(dg2, log="xy", type="l", xlab="Number of vertices", ylab="P(X >= k)")
lines(1:max(filteredDegrees), dg1, col="red")
title("Comparing custom solution with the {igraph} built-in method")
legend("topright",lty=c(1,1),col=c("black", "red"),legend=c("Igraph method","Custom code"))
xmin=1
alpha=2
x=xmin:100000
par(mfrow=c(2,3))
for (grph in graph_list) {
degrees = degree(grph, mode="in")
filteredDegrees = degrees[degrees > 0]
degrees = degree_distribution(selected_grph, cumulative=TRUE, mode="in")
plot(degrees, log="xy", type="l", xlab="Number of vertices", ylab="P(X >= k)")
lines(x, dpldis(x, xmin, alpha), col="red")
lines(x, dpois(x, lambda=1), col="green")
}
title('Complementary cumulative degree distribution (log - log)', line=-1, outer=TRUE)
selected_grph = graph_list[[1]] # We pick one of the graphs in the list, in this case the first
degrees = degree(selected_grph, mode="in")
fit_power_law(degrees)
# in this case, we use filteredDegrees, meaning degrees > 0, as the check_discrete_data method of
# {poweRlaw} only works with strictly positive integers.
m1 = displ$new(filteredDegrees)
# we estimate the value of "xmin" and set it to our function
m1$setXmin(estimate_xmin(m1))
xmin = m1$xmin
alpha = m1$pars
print(xmin)
print(alpha)
n_cores = parallel::detectCores()   # on my local machine the CPU has 12 cores
no_of_sims = 1000
bs = bootstrap_p(m1, no_of_sims=no_of_sims, threads=n_cores)
print(bs$p)
print(bs$p > 0.1)
selected_grph = graph_list[[1]]   # We pick one of the graphs in the list
transitivity(selected_grph)       # global over the whole graph
transitivity(d, type="average")   # same as the previous, but calculated by averaging the local
# coefficients of each vertex
edge_connectivity(selected_grph)    # Connectivity measure for the edges
vertex_connectivity(selected_grph)  # Connectivity measure for the vertices
head(betweenness(selected_grph, directed = TRUE),10)   # Vertices betweenness, first 10 values
head(edge_betweenness(selected_grph, directed = TRUE),10)   # Edges betweenness, first 10 values
skewness = function(x) {
mean( (x - mean(x))^3 ) / sd(x)^3
}
z = degree(selected_grph, mode="in")
print(skewness(z))
print(mean(z) > median(z))
